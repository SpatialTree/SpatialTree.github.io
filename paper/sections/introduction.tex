\begin{figure}[!ht]
\centering
\includegraphics[width=0.98\linewidth]{figures/teaser_tree.pdf}
\caption{\textbf{SpatialTree.} Inspired by cognitive science, our proposed SpatialTree organizes spatial intelligence into a four-layer hierarchy (L1-L4). Rooted in foundational multi-modal capabilities (L0), the tree progressively branches from Basic perception (L1) to agentic competence (L4).}
\label{fig:spatree}
\end{figure}

\footnotetext[1]{Work done during Yuxi Xiao and Longfei Li’s internship at Bytedance Seed.}

\section{Introduction}
\label{sec:intro}

Developing Spatial Intelligence (SI) Systems to perceive, reason, and interact within the physical world is a long-standing challenge across cognitive science~\citep{tolman1948cognitive, shepard1988mental, newcombe2000making}, symbolic AI~\citep{kuipers1978modeling, kuipers2000spatial, harnad1990symbol}, and robotics~\citep{durrant2006simultaneous, thrun2002probabilistic}. However, progress has historically been limited by the lack of a unified model capable of integrating perception, reasoning, and action. The emergence of Multimodal Large Language Models (MLLMs), with their powerful vision-language understanding and reasoning capabilities, has opened new opportunities for advancing SI. 

% --- Full Paragraph based on the "Task Evolve" Roadmap ---
Recent research on spatial intelligence (SI) in MLLMs has largely followed a task-centric trajectory. Early works focused on simple spatial tasks in single images~\citep{ma20243dsrbench, wang2024spatial,fu2024blink}, such as relative object positioning and size estimation. Later studies expanded these tasks to 3D grounding, detection, and captioning from point clouds~\citep{zhu2024llava,hong20233d,xu2024pointllm}. With multi-view and video-capable VLMs, benchmarks quickly diversified~\citep{yang2025thinking, wang2025site, wang2025spatialviz, gholami2025spatial, yang2025mmsi, jia2025omnispatial}, covering a wide array of tasks from spatiotemporal reasoning to egocentric and dynamic object understanding.

However, existing task-centric benchmarks, while useful, remain fragmented—often focusing on isolated or overlapping spatial tasks. This makes it difficult to understand not only an MLLM’s overall spatial intelligence, but also how these abilities emerge and transfer across levels. This motivates us to ask:

\begin{quote}
Can we move beyond fragmented, task-centric evaluations to uncover a compact set of atomic capabilities that reveal how spatial abilities emerge, interact, and transfer?
\end{quote}

Inspired by Piaget’s theory in cognitive science~\cite{piaget2013construction}, we advocate a \emph{capability-centric} paradigm for spatial intelligence (SI). We further decompose SI into a multi-level capability tree (Fig.~\ref{fig:spatree}). Based on this taxonomy, we construct the first comprehensive benchmark for SI in MLLMs, offering comprehensive ability coverage and a diverse set of evaluation metrics beyond simple multiple-choice tests used in prior works. We also develop a Spatial Engine, an extendable annotation framework. It integrates multiple perception models to generate annotations for each capability layer. At the highest level (L4), we propose a spatial action mapping which converts continuous actions into discrete, high-level motion primitives, providing MLLMs with an executable action space for agentic tasks. We leverage the proposed Spatial Engine to generate diverse annotation data from public datasets covering video games, robot manipulation, and human-object interactions, with details provided in Sec.~\ref{sec:l4-agentic-competence}. To cover the lower levels (L1–L3), we extract relevant portions from multiple public datasets and benchmarks~\citep{yang2025mmsi,yang2025thinking,jia2025omnispatial,lin2025towards, wang2025spatialviz, wang2024spatial, zhu2024llava, yin2025spatial, xu2025multi, liu2025ir3d}, reorganizing them onto our capability tree. We further enrich questions and evaluation protocols to improve coverage and cross-layer overlap. To address missing capabilities, we introduce \textbf{SpatialPlus}, generated by \textbf{SpatialEngine}, encompassing Orientation (L1), Memory Retrieval (L2), Relational Reasoning (L3), and Agentic Competence (L4). All resulting data and annotations are systematically organized and re-weighted within the SpatialTree benchmark to ensure balanced evaluation across layers.

% ** 最后一段实验+结论 等搞完之后再写 **
Evaluation on SpatialTree-Bench reveals a clear hierarchical structure in spatial intelligence: low-level abilities (L1–L2) are largely independent, whereas higher-level abilities (L3–L4) exhibit strong interdependencies, reflecting their compositional nature. To systematically validate these correlations, we conduct targeted supervised fine-tuning (SFT) and prompting experiments. Our SFT results reveal a clear cross-level transfer, where training on a single L1 ability boosts high-level L4 tasks despite harming same-level performance. Crucially, we also find a powerful multi-ability synergy: jointly training these foundational L1 abilities unlocks robust holistic gains, far exceeding the negligible or even negative results of individual SFT. We further confirm this dependency via prompting, as explicitly grounding L4 agentic tasks with L1 perceptual cues significantly enhances performance.

In summary, our key contributions are:
\begin{itemize}
    \item We propose the first capability-centric benchmark for spatial intelligence, providing a systematic framework for benchmarking and interpretable analysis.
    \item We design a series of experiments to investigate how abilities emerge and transfer across both same-level and cross-level tasks, revealing the hierarchical relationships from lower- to higher-level capabilities.
\end{itemize}
%\subsection{Hello World}