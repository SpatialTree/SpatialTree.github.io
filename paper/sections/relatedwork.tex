\section{Related Work}
\label{sec:related_work}

\paragraph{Spatial Cognitive Modeling.} 
Understanding spatial cognition has long been a central goal in cognitive science and AI. A common insight from classical theories is that spatial abilities are hierarchical, ranging from basic perception and sensorimotor interactions to higher-level reasoning and planning. Piaget~\cite{piaget2013construction} highlighted the developmental progression of such abilities, Tolman~\cite{tolman1948cognitive} introduced the idea of cognitive maps to represent environments for flexible navigation, and Kuipers~\cite{kuipers1978modeling, kuipers2000spatial} formalized a hierarchical spatial representation linking local perception to global knowledge. More recent symbolic and neural approaches~\cite{shepard1988mental, newcombe2000making} extend these insights to computational models of spatial representation, memory, and reasoning. These studies collectively motivate our \emph{SpatialTree}, which organizes spatial intelligence into multi-level capabilities, bridging classical theory with systematic computational evaluation.


\paragraph{Multi-modal Large Language Models.} 
The success of GPT-3~\cite{brown2020language} and GPT-3.5~\cite{openai-gpt-3.5} demonstrated the potential of large language models for complex linguistic understanding and reasoning. GPT-4V~\cite{openai2023gpt4v} extends GPT-4~\cite{achiam2023gpt} with visual inputs, enabling single-image understanding and basic spatial reasoning. Open-sourced models such as LLaVA~\cite{liu2023visual} and QwenVL~\cite{bai2023qwen} gradually added multi-image and video capabilities, supporting spatiotemporal reasoning. Reasoning-augmented LLMs, pioneered by OpenAI O1~\cite{jaech2024openai} and DeepSeek-R1~\cite{guo2025deepseek}, integrate chain-of-thought and reinforcement learning to enhance high-level inference. Building on these advances, GPT-4O~\cite{hurst2024gpt} and Gemini 2.5~\cite{comanici2025gemini} combine perception and reasoning to support complex, agentic decision-making. Collectively, these milestones progressively enable hierarchical spatial intelligence in MLLMs, motivating structured benchmarks and evaluation frameworks across low-level perception, intermediate reasoning, and high-level agentic competence.



\paragraph{Benchmarks for Spatial Intelligence in MLLMs.} 
Benchmarks for spatial abilities in MLLMs have evolved alongside the models themselves. Early efforts, such as BLINK~\cite{fu2024blink}, SpatialEval~\cite{wang2024spatial}, and 3DSR-Bench~\cite{ma20243dsrbench}, focused on evaluating spatial understanding tasks in single images, including distance estimation, relational question answering, and spatial captions. As MLLMs increasingly support multi-frame and video inputs, benchmarks such as VSI-Bench~\cite{yang2025thinking} and MMSI-Bench~\cite{yang2025mmsi} have emerged to evaluate spatial reasoning across multiple views and dynamic scenes. To further enrich task diversity and coverage, Omnispatial~\cite{jia2025omnispatial}, SITE~\cite{wang2025site}, and IR3D-Bench~\cite{liu2025ir3d} extend benchmarks to geometry puzzles, dynamic reasoning, and inverse rendering tasks. Built upon prior efforts, our SpatialTree benchmark systematically organizes spatial abilities into a hierarchical framework, providing the first thorough evaluation across different capabilities.